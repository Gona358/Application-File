{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gona358/Application-File/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explanation of the intuition behind the Perceptron algorithm."
      ],
      "metadata": {
        "id": "wsu9n5HBPD4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Perceptron algorithm is one of the fundamental building blocks of machine learning and artificial neural networks. It is a simple supervised learning algorithm used for binary classification, meaning it's used to separate data into two distinct classes. Developed by Frank Rosenblatt in the late 1950s, the Perceptron is considered one of the earliest neural network models.\n",
        "\n",
        "Here are the key components and steps of the Perceptron algorithm:\n",
        "\n",
        "1. **Initialization:** The algorithm starts by initializing the weights (often denoted as \"w\") and the bias (also known as the threshold, denoted as \"b\"). These are typically set to small random values or initialized to zeros.\n",
        "\n",
        "2. **Input Data:** For each training example, the algorithm takes a feature vector as input. The feature vector represents the data to be classified.\n",
        "\n",
        "3. **Weighted Sum:** The Perceptron calculates the weighted sum of the input features by multiplying each feature by its corresponding weight and summing them up. The formula for the weighted sum is:\n",
        "\n",
        "   $z = \\sum_{i=1}^{n} w_i \\cdot x_i + b$\n",
        "\n",
        "   where $z$ is the weighted sum, $w_i$ are the weights, $x_i$ are the input features, $n$ is the number of features, and $b$ is the bias.\n",
        "\n",
        "4. **Activation Function:** The weighted sum $z$ is then passed through an activation function. In the classic Perceptron, the activation function is a step function or sign function. If $z$ is greater than or equal to zero, the Perceptron outputs one class (e.g., +1); otherwise, it outputs the other class (e.g., -1). The step function essentially acts as a binary threshold.\n",
        "\n",
        "5. **Updating Weights and Bias:** If the Perceptron makes an incorrect prediction, it updates the weights and bias to minimize the error. The update is based on the misclassified data point, and the algorithm tries to push the decision boundary in the correct direction. The update rule is as follows:\n",
        "\n",
        "   $w_i \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i$\n",
        "  $ b \\leftarrow b + \\alpha \\cdot (y - \\hat{y})$\n",
        "\n",
        "   Where $w_i$ is the weight for feature $i$, $alpha$ is the learning rate (a hyperparameter that controls the step size in weight updates), $y$ is the true label, and $\\hat{y}$ is the predicted label.\n",
        "\n",
        "6. **Iteration:** Steps 3 to 5 are repeated for each data point in the training set. This process continues for a set number of iterations (epochs) or until no misclassifications occur.\n",
        "\n",
        "The Perceptron's main idea is to learn a linear decision boundary that separates two classes. If the data is linearly separable, the Perceptron can find a decision boundary to correctly classify the data. However, if the data is not linearly separable, the Perceptron may not converge to a solution. In such cases, more advanced algorithms like the Support Vector Machine (SVM) are often used.\n",
        "\n",
        "The Perceptron is a simple yet important concept, forming the basis for more complex neural network architectures. While it's not suitable for all types of problems, it played a crucial role in the development of machine learning and the history of artificial intelligence.\n"
      ],
      "metadata": {
        "id": "h_uSr1ghT_JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pseudocode"
      ],
      "metadata": {
        "id": "uFeOaC3jVoRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function PerceptronTraining(training_data, number_of_epochs, learning_rate):\n",
        "    # Initialize weights and bias\n",
        "    for each weight in weights:\n",
        "        weight = random_initial_value()  # You can initialize them randomly or to zero\n",
        "    bias = random_initial_value()  # You can also initialize it randomly or to zero\n",
        "\n",
        "    # Perceptron training\n",
        "    for each epoch in range(number_of_epochs):\n",
        "        for each example in training_data:\n",
        "            # Calculate weighted sum\n",
        "            weighted_sum = 0\n",
        "            for each feature in example:\n",
        "                weighted_sum += weight * feature\n",
        "\n",
        "            weighted_sum += bias\n",
        "\n",
        "            # Apply the activation function (can be the sign function)\n",
        "            prediction = sign_function(weighted_sum)\n",
        "\n",
        "            # Update weights and bias if the prediction is incorrect\n",
        "            if prediction != example.class:\n",
        "                for each weight in weights:\n",
        "                    weight += learning_rate * (example.class - prediction) * feature\n",
        "                bias += learning_rate * (example.class - prediction)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "Function sign_function(value):\n",
        "    if value >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "VRWP1ErFVtMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Algorithm"
      ],
      "metadata": {
        "id": "QDveAucLWrRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(units=1, input_shape=(2,), activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=1000, verbose=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "predictions = model.predict(X)\n",
        "print('Predictions:')\n",
        "for i in range(len(X)):\n",
        "    print(f'Input: {X[i]}, Predicted Output: {predictions[i][0]:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXrq76mcXCC1",
        "outputId": "e7105340-5862-461e-f381-d4d9a08f3bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 127ms/step - loss: 0.7015 - accuracy: 0.5000\n",
            "Loss: 0.7015, Accuracy: 50.00%\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Predictions:\n",
            "Input: [0 0], Predicted Output: 0.41\n",
            "Input: [0 1], Predicted Output: 0.52\n",
            "Input: [1 0], Predicted Output: 0.46\n",
            "Input: [1 1], Predicted Output: 0.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Loss Function and Optimization in Perceptrons\n",
        "\n",
        "In the context of Perceptrons, it's important to discuss the loss function and optimization method used for training.\n",
        "\n",
        "## Loss Function:\n",
        "\n",
        "In Perceptrons, the most commonly used loss function is the mean squared error (MSE) loss, which is defined as:\n",
        "\n",
        "$ L(y, \\hat{y}) = (y - \\hat{y})^2 $\n",
        "\n",
        "Where:\n",
        "- $ L(y, \\hat{y}) $ is the loss function.\n",
        "- $ y $ is the true label of the training example (1 or -1 in binary classification).\n",
        "- $ \\hat{y} $ is the output or prediction of the Perceptron.\n",
        "\n",
        "The goal of learning in the Perceptron is to minimize this loss function, which involves adjusting the weights and bias to make predictions as close as possible to the true labels.\n",
        "\n",
        "## Optimization Method:\n",
        "\n",
        "Perceptrons use a simple form of optimization known as \"Stochastic Gradient Descent\" (SGD). SGD is used to iteratively adjust the weights and bias to minimize the loss function.\n",
        "\n",
        "The learning algorithm for Perceptrons iteratively updates the weights and bias in the direction of the gradient descent of the loss function. The weight and bias updates are defined as follows:\n",
        "\n",
        "$ w_i \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i $\n",
        "$ b \\leftarrow b + \\alpha \\cdot (y - \\hat{y}) $\n",
        "\n",
        "Where:\n",
        "- $ w_i $ is the weight associated with feature $ x_i $.\n",
        "- $ \\alpha $ is the learning rate, controlling the step size for weight updates.\n",
        "- $ y $ is the true label.\n",
        "- $ \\hat{y} $ is the Perceptron's prediction.\n",
        "- $ x_i $ is the $ i $-th feature of the training example.\n",
        "- $ b $ is the bias.\n",
        "\n",
        "The SGD optimization is an iterative process where weights and bias are adjusted to minimize the loss function in each step. This process continues until a fixed number of epochs is reached or until there are no classification errors on the training data.\n"
      ],
      "metadata": {
        "id": "nvNk66XdYNrO"
      }
    }
  ]
}