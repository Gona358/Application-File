{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gona358/Application-File/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intuition behind the KNN algorithm:\n",
        "The k-Nearest Neighbors (k-NN) algorithm is a supervised learning algorithm used for both classification and regression problems. Its intuition is based on the idea that similar data points tend to belong to the same class or have similar output values.\n",
        "\n",
        "The central idea is that if a data point is similar to other nearby points in the feature space, it is likely to belong to the same class or have similar output values. For classification, the algorithm looks for the \"k\" nearest training points to a test point and predicts the majority class among those neighbors. For regression, it averages the output values of the \"k\" nearest neighbors as the prediction.\n",
        "\n",
        "The KNN algorithm is a widely used and intuitive machine learning algorithm that falls under the category of supervised learning. Its primary purpose is to perform both classification and regression tasks by relying on the concept of proximity in feature space. In this report, we delve into the fundamental intuition behind the KNN algorithm, outlining its key concepts and providing practical insights.\n",
        "\n",
        "1. Proximity-Based Decision Making\n",
        "At the core of the KNN algorithm lies the principle that similar data points tend to exhibit similar characteristics or outcomes. This intuition is based on the observation that items or entities in a dataset often cluster together in feature space, making it plausible to infer the properties of an unknown data point by examining its proximity to known data points.\n",
        "\n",
        "2. Localized Decision Boundaries\n",
        "KNN operates under the assumption that decision boundaries, which separate different classes or predict output values, are not necessarily linear or rigid. Instead, it suggests that these boundaries can be highly localized and adapt to the distribution of data. This flexibility makes KNN particularly well-suited for scenarios where decision boundaries are complex and non-linear.\n",
        "\n",
        "3. Nearest Neighbor Voting\n",
        "In the case of classification, KNN employs a straightforward \"voting\" mechanism to determine the class of a test data point. Specifically, for a given test point, KNN calculates the distances to all training data points and selects the K nearest neighbors. The class that appears most frequently among these neighbors is assigned as the predicted class for the test point. This approach capitalizes on the idea that a majority of similar neighbors implies the same class.\n",
        "\n",
        "4. Regression by Averaging\n",
        "For regression tasks, KNN adopts a similar approach but focuses on predicting continuous values instead of classes. Instead of voting, KNN calculates the average (or weighted average) of the output values of the K nearest neighbors and assigns this value as the prediction. This technique leverages the assumption that similar data points tend to have similar output values.\n",
        "\n",
        "5. Hyperparameter K\n",
        "A crucial aspect of KNN is the choice of the hyperparameter \"K,\" which represents the number of nearest neighbors to consider. The value of K significantly impacts the algorithm's performance. A smaller K may lead to noisy predictions that are sensitive to individual data points, while a larger K can result in smoother but potentially biased predictions.\n",
        "\n",
        "**Practical Implications\n",
        "Understanding the intuition behind KNN has practical implications for its application:**\n",
        "\n",
        "*Data Preprocessing:* KNN is sensitive to the scale and relevance of features. Data preprocessing techniques such as normalization and feature selection can enhance its performance.\n",
        "\n",
        "*Choosing K:* Selecting an appropriate value for K is often determined through experimentation and cross-validation. It involves a trade-off between model bias and variance.\n",
        "\n",
        "*Distance Metric:* The choice of distance metric (e.g., Euclidean, Manhattan, etc.) should align with the characteristics of the data and the problem being solved.\n",
        "\n",
        "*Efficiency:* KNN can be computationally intensive, especially with large datasets. Techniques like KD-trees or ball trees can be employed to speed up nearest neighbor search."
      ],
      "metadata": {
        "id": "hmZqKBmLVESH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pseudocode for the KNN algorithm:"
      ],
      "metadata": {
        "id": "58UNNQKcVi-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "For each test point X in the test set:\n",
        "    Calculate the distance between X and all training points.\n",
        "    Select the k nearest training points to X.\n",
        "    For classification:\n",
        "        Determine the majority class among the k neighbors and assign it to X.\n",
        "    For regression:\n",
        "        Calculate the average output value of the k neighbors and assign it to X.\n"
      ],
      "metadata": {
        "id": "Xk-SkY0oVgj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of the Pseudocode algorithm in Python:"
      ],
      "metadata": {
        "id": "XYbNEoDNVtsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict_classification(self, X_test):\n",
        "        y_pred = [self._predict_class(x) for x in X_test]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict_class(self, x):\n",
        "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n",
        "\n",
        "class KNNRegressor:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict_regression(self, X_test):\n",
        "        y_pred = [self._predict_value(x) for x in X_test]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict_value(self, x):\n",
        "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_values = [self.y_train[i] for i in k_indices]\n",
        "        return np.mean(k_nearest_values)\n",
        "\n",
        "X_train_cls = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y_train_cls = np.array([0, 0, 1, 1])\n",
        "X_test_cls = np.array([[2.5, 3.5]])\n",
        "\n",
        "knn_classifier = KNNClassifier(k=2)\n",
        "knn_classifier.fit(X_train_cls, y_train_cls)\n",
        "y_pred_cls = knn_classifier.predict_classification(X_test_cls)\n",
        "print(\"Clase predicha (clasificación):\", y_pred_cls)\n",
        "\n",
        "X_train_reg = np.array([[1], [2], [3], [4]])\n",
        "y_train_reg = np.array([1, 2, 3, 4])\n",
        "X_test_reg = np.array([[2.5]])\n",
        "\n",
        "knn_regressor = KNNRegressor(k=2)\n",
        "knn_regressor.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg = knn_regressor.predict_regression(X_test_reg)\n",
        "print(\"Valor predicho (regresión):\", y_pred_reg)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2XYVKPqVu3u",
        "outputId": "edbd2f76-6289-480d-b86b-e3400951ae1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase predicha (clasificación): [0]\n",
            "Valor predicho (regresión): [2.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss and Optimization Functions:\n",
        "In the KNN algorithm, there is no specific loss function or optimization process like in traditional supervised learning algorithms (e.g., linear regression, SVM, etc.). KNN does not learn parameters during training; it simply memorizes the training data and uses it to make predictions based on proximity in the feature space. Therefore, there is no loss function to minimize or parameter optimization.\n",
        "\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a non-parametric and instance-based machine learning algorithm used for both classification and regression tasks. Unlike many other machine learning algorithms, KNN does not involve explicit loss functions or optimization procedures during training. In this report, we will discuss why KNN does not have these components and how it differs from traditional supervised learning algorithms in this regard.\n",
        "\n",
        "**Loss Functions in Traditional Machine Learning:**\n",
        "In many supervised learning algorithms, such as linear regression, logistic regression, and support vector machines, a loss function is defined to measure the error or discrepancy between the model's predictions and the true labels (in classification) or target values (in regression). The goal of training these models is to minimize this loss function, which essentially quantifies how well the model fits the training data.\n",
        "\n",
        "For example, in linear regression, the Mean Squared Error (MSE) is a common loss function:\n",
        "**MSE = (1/N) ∑(y_i - ŷ_i)^2**\n",
        "\n",
        "In this formula, y_i represents the true target values, ŷ_i represents the predicted values, and N is the number of data points. The goal is to minimize this MSE by adjusting the model parameters (weights and biases).\n",
        "\n",
        "**Optimization Functions in Traditional Machine Learning:**\n",
        "To minimize the loss function, optimization algorithms like gradient descent or stochastic gradient descent (SGD) are typically used. These optimization techniques iteratively adjust the model's parameters to find the values that lead to the lowest loss. The gradients of the loss function with respect to the model parameters are used to update the parameters in the direction that reduces the loss.\n",
        "\n",
        "**KNN's Distinct Approach:**\n",
        "KNN takes a fundamentally different approach compared to traditional supervised learning algorithms. It does not involve a predefined loss function to minimize or an optimization procedure to adjust model parameters. Instead, KNN memorizes the training data and performs predictions solely based on the distances between data points.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HPf3PitYkW_"
      }
    }
  ]
}